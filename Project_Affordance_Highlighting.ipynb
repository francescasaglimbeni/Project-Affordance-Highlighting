{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francescasaglimbeni/Project-Affordance-Highlighting/blob/main/Project_Affordance_Highlighting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PROJECT**"
      ],
      "metadata": {
        "id": "e4K1R9TxIDYe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naVQV8DySmsj",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install kaolin==0.17.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html\n",
        "!pip install transforms3d\n",
        "!pip install gdown\n",
        "!pip install open3d\n",
        "!pip install torch\n",
        "!pip install numpy\n",
        "!pip install tqdm\n",
        "!pip install clip\n",
        "!pip install pytorch3d"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pathlib\n",
        "import copy\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import clip\n",
        "import kaolin as kal\n",
        "import kaolin.ops.mesh\n",
        "import torchvision\n",
        "import open3d as o3d\n",
        "import gdown\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Optional, Union\n",
        "from pathlib import Path\n",
        "from os.path import join as opj\n",
        "import h5py\n",
        "import pickle as pkl\n",
        "import shutil\n",
        "from scipy.spatial import KDTree"
      ],
      "metadata": {
        "id": "h-NQBz3Wam7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/output_PART3\n",
        "!rm -rf /content/Affordance_Highlighting_Project_2024/output"
      ],
      "metadata": {
        "id": "2gs70pwozvPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creazione cartella progetto + definizione rete + clip loss**"
      ],
      "metadata": {
        "id": "jsyOUGdyIIBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_path = pathlib.Path('./Affordance_Highlighting_Project_2024')\n",
        "if not repo_path.is_dir():\n",
        "    os.system('git clone https://github.com/paolotron/Affordance_Highlighting_Project_2024.git')\n",
        "\n",
        "%cd ./Affordance_Highlighting_Project_2024/\n",
        "!ls"
      ],
      "metadata": {
        "id": "tG8iKBITaq26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HVZGHlPSmsk"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "from collections import defaultdict\n",
        "from itertools import permutations, product\n",
        "from Normalization import MeshNormalizer\n",
        "from mesh import Mesh\n",
        "from pathlib import Path\n",
        "from render import Renderer\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import grad\n",
        "from torchvision import transforms\n",
        "from utils import device, color_mesh\n",
        "\n",
        "\n",
        "class FourierFeatureTransform(nn.Module):\n",
        "    def __init__(self, input_dim, width, sigma):\n",
        "        super(FourierFeatureTransform, self).__init__()\n",
        "        self.sigma = sigma\n",
        "        self.linear = nn.Linear(input_dim, width)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = torch.sin(x * self.sigma)\n",
        "        return x\n",
        "\n",
        "class NeuralHighlighter(nn.Module):\n",
        "    def __init__(self,  out_dim = 2, depth = 4, width=256, input_dim=3, positional_encoding=False, sigma=5.0):\n",
        "        super(NeuralHighlighter, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        if positional_encoding:\n",
        "            layers.append(FourierFeatureTransform(input_dim, width, sigma))\n",
        "            layers.append(nn.Linear(width * 2 + input_dim, width))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.LayerNorm([width]))\n",
        "        else:\n",
        "            layers.append(nn.Linear(input_dim, width))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.LayerNorm([width]))\n",
        "\n",
        "\n",
        "        for i in range(depth):\n",
        "            layers.append(nn.Linear(width, width))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.LayerNorm([width]))\n",
        "\n",
        "        layers.append(nn.Linear(width, out_dim))\n",
        "        layers.append(nn.Softmax(dim=1))\n",
        "\n",
        "        self.mlp = nn.ModuleList(layers)\n",
        "        print(self.mlp)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.mlp:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "def get_clip_model(clip_model):\n",
        "    # Usa open-clip per caricare il modello\n",
        "    clip_model, preprocess = clip.load(clip_model, device=device)\n",
        "    return clip_model, preprocess"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_loss(rendered_images, clip_model, encoded_text, clip_transform, augment_transform,  n_augs, clipavg):\n",
        "    if n_augs == 0:\n",
        "        clip_image = clip_transform(rendered_images)\n",
        "        encoded_renders = clip_model.encode_image(clip_image)\n",
        "        encoded_renders = encoded_renders / encoded_renders.norm(dim=1, keepdim=True)\n",
        "        if clipavg == \"view\":\n",
        "            if encoded_text.shape[0] > 1:\n",
        "                loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
        "                                                torch.mean(encoded_text, dim=0), dim=0)\n",
        "            else:\n",
        "                loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
        "                                                encoded_text)\n",
        "        else:\n",
        "            loss = torch.mean(torch.cosine_similarity(encoded_renders, encoded_text))\n",
        "    elif n_augs > 0:\n",
        "        loss = 0.0\n",
        "        for _ in range(n_augs):\n",
        "            augmented_image = augment_transform(rendered_images)\n",
        "            encoded_renders = clip_model.encode_image(augmented_image)\n",
        "            if clipavg == \"view\":\n",
        "                if encoded_text.shape[0] > 1:\n",
        "                    loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
        "                                                    torch.mean(encoded_text, dim=0), dim=0)\n",
        "                else:\n",
        "                    loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
        "                                                    encoded_text)\n",
        "            else:\n",
        "                loss -= torch.mean(torch.cosine_similarity(encoded_renders, encoded_text))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "ICBuK38Sb6BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background):\n",
        "    mlp.eval()\n",
        "    with torch.no_grad():\n",
        "        probs = mlp(vertices)\n",
        "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
        "        # for renders\n",
        "        one_hot = torch.zeros(probs.shape).to(device)\n",
        "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
        "        sampled_mesh = mesh\n",
        "\n",
        "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
        "        gray = torch.tensor([180, 180, 180]).to(device)\n",
        "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
        "        color_mesh(one_hot, sampled_mesh, colors)\n",
        "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=5,\n",
        "                                                                        show=False,\n",
        "                                                                        center_azim=0,\n",
        "                                                                        center_elev=0,\n",
        "                                                                        std=1,\n",
        "                                                                        return_views=True,\n",
        "                                                                        lighting=True,\n",
        "                                                                        background=background)\n",
        "        # for mesh\n",
        "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
        "        final_color = torch.where(max_idx==0, highlight, gray)\n",
        "        mesh.export(os.path.join(log_dir, f\"{name}.ply\"), extension=\"ply\", color=final_color)\n",
        "        save_renders(log_dir, 0, rendered_images, name='final_render.jpg')"
      ],
      "metadata": {
        "id": "uZaNJQz7b_-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_renders(dir, i, rendered_images, name=None):\n",
        "    if name is not None:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
        "    else:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))"
      ],
      "metadata": {
        "id": "5j3Ort_lb_hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PART 1 - PIPELINE**"
      ],
      "metadata": {
        "id": "m0lV2lKAIQFC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gqkPNOJSmsl",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "render_res = 224\n",
        "learning_rate = 0.0001\n",
        "n_iter = 2500\n",
        "res = 224\n",
        "obj_path = '/content/Affordance_Highlighting_Project_2024/data/dog.obj'\n",
        "n_augs = 5\n",
        "output_dir = './output/'\n",
        "clip_model = 'ViT-B/32'\n",
        "clipavg = 'view'\n",
        "\n",
        "Path(os.path.join(output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "objbase, extension = os.path.splitext(os.path.basename(obj_path))\n",
        "\n",
        "render = Renderer(dim=(render_res, render_res))\n",
        "mesh = Mesh(obj_path)\n",
        "MeshNormalizer(mesh)()\n",
        "\n",
        "clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "clip_transform = transforms.Compose([\n",
        "       transforms.Resize((res, res)),\n",
        "       clip_normalizer\n",
        "   ])\n",
        "augment_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(res, scale=(1, 1)),\n",
        "        transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
        "        clip_normalizer\n",
        "    ])\n",
        "\n",
        "background = torch.tensor((1., 1., 1.)).to(device)\n",
        "\n",
        "log_dir = output_dir\n",
        "\n",
        "mlp = NeuralHighlighter().to(device)\n",
        "optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n",
        "\n",
        "rgb_to_color = {(204/255, 1., 0.): \"highlighter\", (180/255, 180/255, 180/255): \"gray\"}\n",
        "color_to_rgb = {\"highlighter\": [204/255, 1., 0.], \"gray\": [180/255, 180/255, 180/255]}\n",
        "full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]\n",
        "colors = torch.tensor(full_colors).to(device)\n",
        "\n",
        "\n",
        "\n",
        "clip_model, preprocess = get_clip_model(clip_model)\n",
        "#PROMPT:\n",
        "prompt = \"A 3D render of a gray dog with highlighted hat\"\n",
        "with torch.no_grad():\n",
        "        prompt_token = clip.tokenize([prompt]).to(device)\n",
        "        encoded_text = clip_model.encode_text(prompt_token)\n",
        "        encoded_text = encoded_text / encoded_text.norm(dim=1, keepdim=True)\n",
        "\n",
        "name_p = 'dog'\n",
        "vertices = copy.deepcopy(mesh.vertices)\n",
        "\n",
        "n_views = 5\n",
        "\n",
        "losses = []\n",
        "\n",
        "# OOPTIMIZED LOOP:\n",
        "for i in tqdm(range(n_iter)):\n",
        "    optim.zero_grad()\n",
        "\n",
        "    pred_class = mlp(vertices)\n",
        "\n",
        "    sampled_mesh = mesh\n",
        "    color_mesh(pred_class, sampled_mesh, colors)\n",
        "    rendered_images, elev, azim = render.render_views(sampled_mesh, num_views=n_views,\n",
        "                                                            show=False,\n",
        "                                                            center_azim=0,\n",
        "                                                            center_elev=0,\n",
        "                                                            std=1,\n",
        "                                                            return_views=True,\n",
        "                                                            lighting=True,\n",
        "                                                            background=background)\n",
        "\n",
        "    # Calculate CLIP Loss\n",
        "    loss = clip_loss(rendered_images, clip_model, encoded_text, clip_transform, augment_transform, n_augs, clipavg)\n",
        "    loss.backward(retain_graph=True)\n",
        "\n",
        "    optim.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print(\"Last 100 CLIP score: {}\".format(np.mean(losses[-100:])))\n",
        "        save_renders(log_dir, i, rendered_images)\n",
        "        with open(os.path.join(log_dir, \"training_info.txt\"), \"a\") as f:\n",
        "            f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n",
        "\n",
        "save_final_results(log_dir, name_p, mesh, mlp, vertices, colors, render, background)\n",
        "\n",
        "with open(os.path.join(log_dir, 'prompt.txt'), \"w\") as f:\n",
        "    f.write('')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PART 2 - PIPELINE**"
      ],
      "metadata": {
        "id": "du53-mA3cwVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **POINTCLOUD TO MESH**"
      ],
      "metadata": {
        "id": "B3rUYlKrc6yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_directory = './pointcloud'\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "mesh = o3d.io.read_triangle_mesh(\"/content/Affordance_Highlighting_Project_2024/data/horse.obj\")\n",
        "\n",
        "point_cloud = mesh.sample_points_uniformly(number_of_points=50000)\n",
        "\n",
        "point_cloud.remove_statistical_outlier(nb_neighbors=25, std_ratio=2.5)\n",
        "\n",
        "point_cloud.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=40))\n",
        "\n",
        "o3d.visualization.draw_geometries([point_cloud])\n",
        "\n",
        "ply_output_path = os.path.join(output_directory, \"horse.ply\")\n",
        "o3d.io.write_point_cloud(ply_output_path, point_cloud)\n",
        "\n",
        "points = np.asarray(point_cloud.points)\n",
        "\n",
        "kdtree = KDTree(points)\n",
        "radius = 0.02\n",
        "densities = np.array([len(kdtree.query_ball_point(p, radius)) for p in points])\n",
        "\n",
        "densities_normalized = (densities - densities.min()) / (densities.max() - densities.min())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Aji9cdCOczLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def point_cloud_to_mesh(ply_path, obj_output_path, depth=9):\n",
        "    pcd = o3d.io.read_point_cloud(ply_path)\n",
        "    print(f\"Loaded point cloud with {len(pcd.points)} points.\")\n",
        "\n",
        "\n",
        "    mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(pcd, depth=depth)\n",
        "    densities = np.asarray(densities)\n",
        "    vertices_to_remove = densities < np.quantile(densities, 0.001)\n",
        "    mesh.remove_vertices_by_mask(vertices_to_remove)\n",
        "\n",
        "    # Compute vertex normals\n",
        "    mesh.compute_vertex_normals()\n",
        "    mesh = mesh.filter_smooth_simple(number_of_iterations=5)\n",
        "\n",
        "    o3d.io.write_triangle_mesh(obj_output_path, mesh)\n",
        "    print(f\"Mesh saved to {obj_output_path}\")\n",
        "\n",
        "\n",
        "point_cloud_path = \"/content/Affordance_Highlighting_Project_2024/pointcloud/horse.ply\"\n",
        "\n",
        "##where is the mesh that we have to use in pipeline:\n",
        "mesh_output_path = \"/content/Affordance_Highlighting_Project_2024/data/point_cloud.obj\"\n",
        "\n",
        "point_cloud_to_mesh(point_cloud_path, mesh_output_path, depth = 9)"
      ],
      "metadata": {
        "id": "MPkQAyP9dQOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **PIPELINE**"
      ],
      "metadata": {
        "id": "nf1Ocm3ddo6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "render_res = 224\n",
        "learning_rate = 0.0001\n",
        "n_iter = 2500\n",
        "res = 224\n",
        "obj_path = '/content/Affordance_Highlighting_Project_2024/data/point_cloud.obj'\n",
        "n_augs = 5\n",
        "output_dir = './output/'\n",
        "clip_model = 'ViT-B/32'\n",
        "clipavg = 'view'\n",
        "\n",
        "Path(os.path.join(output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "objbase, extension = os.path.splitext(os.path.basename(obj_path))\n",
        "\n",
        "render = Renderer(dim=(render_res, render_res))\n",
        "mesh = Mesh(obj_path)\n",
        "MeshNormalizer(mesh)()\n",
        "\n",
        "clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "clip_transform = transforms.Compose([\n",
        "       transforms.Resize((res, res)),\n",
        "       clip_normalizer\n",
        "   ])\n",
        "augment_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(res, scale=(1, 1)),\n",
        "        transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
        "        clip_normalizer\n",
        "    ])\n",
        "\n",
        "background = torch.tensor((1., 1., 1.)).to(device)\n",
        "\n",
        "log_dir = output_dir\n",
        "\n",
        "mlp = NeuralHighlighter().to(device)\n",
        "optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n",
        "\n",
        "rgb_to_color = {(204/255, 1., 0.): \"highlighter\", (180/255, 180/255, 180/255): \"gray\"}\n",
        "color_to_rgb = {\"highlighter\": [204/255, 1., 0.], \"gray\": [180/255, 180/255, 180/255]}\n",
        "full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]\n",
        "colors = torch.tensor(full_colors).to(device)\n",
        "\n",
        "clip_model, preprocess = get_clip_model(clip_model)\n",
        "\n",
        "# PROMPT:\n",
        "prompt = \"A 3D render of a grey horse with highlighted necklace\"\n",
        "with torch.no_grad():\n",
        "        prompt_token = clip.tokenize([prompt]).to(device)\n",
        "        encoded_text = clip_model.encode_text(prompt_token)\n",
        "        encoded_text = encoded_text / encoded_text.norm(dim=1, keepdim=True)\n",
        "\n",
        "name_p = 'horse'\n",
        "vertices = copy.deepcopy(mesh.vertices)\n",
        "\n",
        "n_views = 5\n",
        "\n",
        "losses = []\n",
        "\n",
        "#OPTIMIZATION LOOP:\n",
        "for i in tqdm(range(n_iter)):\n",
        "    optim.zero_grad()\n",
        "\n",
        "    pred_class = mlp(vertices)\n",
        "\n",
        "    sampled_mesh = mesh\n",
        "    color_mesh(pred_class, sampled_mesh, colors)\n",
        "    rendered_images, elev, azim = render.render_views(sampled_mesh, num_views=n_views,\n",
        "                                                            show=False,\n",
        "                                                            center_azim=0,\n",
        "                                                            center_elev=0,\n",
        "                                                            std=1,\n",
        "                                                            return_views=True,\n",
        "                                                            lighting=True,\n",
        "                                                            background=background)\n",
        "\n",
        "    # Calculate CLIP Loss\n",
        "    loss = clip_loss(rendered_images, clip_model, encoded_text, clip_transform, augment_transform, n_augs, clipavg)\n",
        "    loss.backward(retain_graph=True)\n",
        "\n",
        "    optim.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    # report results\n",
        "    if i % 100 == 0:\n",
        "        print(\"Last 100 CLIP score: {}\".format(np.mean(losses[-100:])))\n",
        "        save_renders(log_dir, i, rendered_images)\n",
        "\n",
        "        with open(os.path.join(log_dir, \"training_info.txt\"), \"a\") as f:\n",
        "            f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n",
        "\n",
        "save_final_results(log_dir, name_p, mesh, mlp, vertices, colors, render, background)\n",
        "\n",
        "with open(os.path.join(log_dir, 'prompt.txt'), \"w\") as f:\n",
        "    f.write('')"
      ],
      "metadata": {
        "id": "R4kQQ4r5dofV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PART 3 - PIPELINE**"
      ],
      "metadata": {
        "id": "LfpzkRDQeJAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **AFFORDANCENET + DATASET**"
      ],
      "metadata": {
        "id": "dWZHmwiveQiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_path = pathlib.Path('AffordanceNet')\n",
        "if not repo_path.is_dir():\n",
        "    os.system('git clone https://github.com/Gorilla-Lab-SCUT/AffordanceNet.git')"
      ],
      "metadata": {
        "id": "1Wgx9V20eKyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_id = '1siZtGusB1LfQVapTvNOiYi8aeKKAgcDF'\n",
        "\n",
        "url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
        "\n",
        "gdown.download(url, 'dataset.zip', quiet=False)\n",
        "!unzip dataset.zip -d /content/dataset"
      ],
      "metadata": {
        "id": "wq09GzCoeIao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to move provider.py to solve problems with import:"
      ],
      "metadata": {
        "id": "wiCgCrQdeZS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source = \"/content/Affordance_Highlighting_Project_2024/AffordanceNet/utils/provider.py\"\n",
        "\n",
        "destination = \"/content/Affordance_Highlighting_Project_2024\"\n",
        "\n",
        "os.makedirs(destination, exist_ok=True)\n",
        "\n",
        "try:\n",
        "    shutil.move(source, destination)\n",
        "    print(f\"File moved in: {destination}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error file doesn't exist\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "cczDQ2WDedsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Affordance_Highlighting_Project_2024\n",
        "from provider import rotate_point_cloud_SO3, rotate_point_cloud_y\n",
        "\n",
        "def pc_normalize(pc):\n",
        "    centroid = np.mean(pc, axis=0)\n",
        "    pc = pc - centroid\n",
        "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
        "    pc = pc / m\n",
        "    return pc, centroid, m\n",
        "\n",
        "\n",
        "def semi_points_transform(points):\n",
        "    spatialExtent = np.max(points, axis=0) - np.min(points, axis=0)\n",
        "    eps = 2e-3*spatialExtent[np.newaxis, :]\n",
        "    jitter = eps*np.random.randn(points.shape[0], points.shape[1])\n",
        "    points_ = points + jitter\n",
        "    return points_\n",
        "\n",
        "\n",
        "class AffordNetDataset(Dataset):\n",
        "    def __init__(self, data_dir, split, partial=False, rotate='None', semi=False):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.split = split\n",
        "\n",
        "        self.partial = partial\n",
        "        self.rotate = rotate\n",
        "        self.semi = semi\n",
        "\n",
        "        self.load_data()\n",
        "\n",
        "        self.affordance = self.all_data[0][\"affordance\"]\n",
        "\n",
        "        return\n",
        "\n",
        "    def load_data(self):\n",
        "        self.all_data = []\n",
        "        if self.semi:\n",
        "            with open(opj(self.data_dir, 'semi_label_1.pkl'), 'rb') as f:\n",
        "                temp_data = pkl.load(f)\n",
        "        else:\n",
        "            if self.partial:\n",
        "                with open(opj(self.data_dir, 'partial_%s_data.pkl' % self.split), 'rb') as f:\n",
        "                    temp_data = pkl.load(f)\n",
        "            elif self.rotate != \"None\" and self.split != 'train':\n",
        "                with open(opj(self.data_dir, 'rotate_%s_data.pkl' % self.split), 'rb') as f:\n",
        "                    temp_data_rotate = pkl.load(f)\n",
        "                with open(opj(self.data_dir, 'full_shape_%s_data.pkl' % self.split), 'rb') as f:\n",
        "                    temp_data = pkl.load(f)\n",
        "            else:\n",
        "                with open(opj(self.data_dir, 'full_shape_%s_data.pkl' % self.split), 'rb') as f:\n",
        "                    temp_data = pkl.load(f)\n",
        "        for index, info in enumerate(temp_data):\n",
        "            if self.partial:\n",
        "                partial_info = info[\"partial\"]\n",
        "                for view, data_info in partial_info.items():\n",
        "                    temp_info = {}\n",
        "                    temp_info[\"shape_id\"] = info[\"shape_id\"]\n",
        "                    temp_info[\"semantic class\"] = info[\"semantic class\"]\n",
        "                    temp_info[\"affordance\"] = info[\"affordance\"]\n",
        "                    temp_info[\"view_id\"] = view\n",
        "                    temp_info[\"data_info\"] = data_info\n",
        "                    self.all_data.append(temp_info)\n",
        "            elif self.split != 'train' and self.rotate != 'None':\n",
        "                rotate_info = temp_data_rotate[index][\"rotate\"][self.rotate]\n",
        "                full_shape_info = info[\"full_shape\"]\n",
        "                for r, r_data in rotate_info.items():\n",
        "                    temp_info = {}\n",
        "                    temp_info[\"shape_id\"] = info[\"shape_id\"]\n",
        "                    temp_info[\"semantic class\"] = info[\"semantic class\"]\n",
        "                    temp_info[\"affordance\"] = info[\"affordance\"]\n",
        "                    temp_info[\"data_info\"] = full_shape_info\n",
        "                    temp_info[\"rotate_matrix\"] = r_data.astype(np.float32)\n",
        "                    self.all_data.append(temp_info)\n",
        "            else:\n",
        "                temp_info = {}\n",
        "                temp_info[\"shape_id\"] = info[\"shape_id\"]\n",
        "                temp_info[\"semantic class\"] = info[\"semantic class\"]\n",
        "                temp_info[\"affordance\"] = info[\"affordance\"]\n",
        "                temp_info[\"data_info\"] = info[\"full_shape\"]\n",
        "                self.all_data.append(temp_info)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        data_dict = self.all_data[index]\n",
        "        modelid = data_dict[\"shape_id\"]\n",
        "        modelcat = data_dict[\"semantic class\"]\n",
        "\n",
        "        data_info = data_dict[\"data_info\"]\n",
        "        model_data = data_info[\"coordinate\"].astype(np.float32)\n",
        "        labels = data_info[\"label\"]\n",
        "        for aff in self.affordance:\n",
        "            temp = labels[aff].astype(np.float32).reshape(-1, 1)\n",
        "            model_data = np.concatenate((model_data, temp), axis=1)\n",
        "\n",
        "        datas = model_data[:, :3]\n",
        "        targets = model_data[:, 3:]\n",
        "\n",
        "        if self.rotate != 'None':\n",
        "            if self.split == 'train':\n",
        "                if self.rotate == 'so3':\n",
        "                    datas = rotate_point_cloud_SO3(\n",
        "                        datas[np.newaxis, :, :]).squeeze()\n",
        "                elif self.rotate == 'z':\n",
        "                    datas = rotate_point_cloud_y(\n",
        "                        datas[np.newaxis, :, :]).squeeze()\n",
        "            else:\n",
        "                r_matrix = data_dict[\"rotate_matrix\"]\n",
        "                datas = (np.matmul(r_matrix, datas.T)).T\n",
        "\n",
        "        datas, _, _ = pc_normalize(datas)\n",
        "\n",
        "        return datas, datas, targets, modelid, modelcat\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_data)"
      ],
      "metadata": {
        "id": "EtAoOwLXeoD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **PIPELINE**"
      ],
      "metadata": {
        "id": "CKG3lvkKerYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_highlighted_regions(\n",
        "    obj_path,\n",
        "    prompt,\n",
        "    output_dir='./output/',\n",
        "    clip_model_name='ViT-B/32',\n",
        "    render_res=224,\n",
        "    res=224,\n",
        "    learning_rate=0.0001,\n",
        "    n_iter=2500,\n",
        "    n_views=5,\n",
        "    n_augs=5,\n",
        "    device='cuda',\n",
        "    mesh_id=None,\n",
        "    is_validation=False\n",
        "):\n",
        "\n",
        "    seed = 0\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    os.makedirs(os.path.join(output_dir, 'renders'), exist_ok=True)\n",
        "\n",
        "    objbase, extension = os.path.splitext(os.path.basename(obj_path))\n",
        "\n",
        "    render = Renderer(dim=(render_res, render_res))\n",
        "    mesh = Mesh(obj_path)\n",
        "    MeshNormalizer(mesh)()\n",
        "\n",
        "    clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "    clip_transform = transforms.Compose([\n",
        "        transforms.Resize((res, res)),\n",
        "        clip_normalizer\n",
        "    ])\n",
        "    augment_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(res, scale=(1, 1)),\n",
        "        transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
        "        clip_normalizer\n",
        "    ])\n",
        "\n",
        "    clip_model, preprocess = get_clip_model(clip_model_name)\n",
        "    with torch.no_grad():\n",
        "        prompt_token = clip.tokenize([prompt]).to(device)\n",
        "        encoded_text = clip_model.encode_text(prompt_token)\n",
        "        encoded_text = encoded_text / encoded_text.norm(dim=1, keepdim=True)\n",
        "\n",
        "    mlp = NeuralHighlighter().to(device)\n",
        "    optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n",
        "\n",
        "    rgb_to_color = {(204/255, 1., 0.): \"highlighter\", (180/255, 180/255, 180/255): \"gray\"}\n",
        "    color_to_rgb = {\"highlighter\": [204/255, 1., 0.], \"gray\": [180/255, 180/255, 180/255]}\n",
        "    full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]\n",
        "    colors = torch.tensor(full_colors).to(device)\n",
        "\n",
        "    background = torch.tensor((1., 1., 1.)).to(device)\n",
        "\n",
        "    vertices = copy.deepcopy(mesh.vertices)\n",
        "    losses = []\n",
        "\n",
        "    # OPTIMIZATION LOOP:\n",
        "    for i in tqdm(range(n_iter)):\n",
        "        optim.zero_grad()\n",
        "\n",
        "        pred_class = mlp(vertices)\n",
        "\n",
        "        sampled_mesh = mesh\n",
        "        color_mesh(pred_class, sampled_mesh, colors)\n",
        "        rendered_images, elev, azim = render.render_views(sampled_mesh, num_views=n_views,\n",
        "                                                          show=False,\n",
        "                                                          center_azim=0,\n",
        "                                                          center_elev=0,\n",
        "                                                          std=1,\n",
        "                                                          return_views=True,\n",
        "                                                          lighting=True,\n",
        "                                                          background=background)\n",
        "\n",
        "        loss = clip_loss(rendered_images, clip_model, encoded_text, clip_transform, augment_transform, n_augs, clipavg='view')\n",
        "        loss.backward(retain_graph=True)\n",
        "\n",
        "        optim.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        if is_validation and i % 100 == 0:\n",
        "            save_renders_PART3(output_dir, i, rendered_images, mesh_id=mesh_id)\n",
        "            with open(os.path.join(output_dir, \"training_info.txt\"), \"a\") as f:\n",
        "                f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n",
        "\n",
        "    if is_validation:\n",
        "        save_final_results_PART3(output_dir, objbase, mesh, mlp, vertices, colors, render, background, mesh_id = mesh_id)\n",
        "\n",
        "\n",
        "    return pred_class"
      ],
      "metadata": {
        "id": "PK8bqsTpett3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def point_cloud_to_mesh_part3(points, output_path):\n",
        "    try:\n",
        "        pcd = o3d.geometry.PointCloud()\n",
        "        pcd.points = o3d.utility.Vector3dVector(points)\n",
        "\n",
        "        cl, ind = pcd.remove_statistical_outlier(nb_neighbors=25, std_ratio=2.5)\n",
        "        pcd = pcd.select_by_index(ind)\n",
        "\n",
        "        pcd = pcd.voxel_down_sample(voxel_size=0.003)\n",
        "\n",
        "        pcd.estimate_normals(\n",
        "            search_param=o3d.geometry.KDTreeSearchParamHybrid(\n",
        "                radius=0.1,\n",
        "                max_nn=40\n",
        "            )\n",
        "        )\n",
        "\n",
        "        alpha = 0.05\n",
        "        mesh = o3d.geometry.TriangleMesh.create_from_point_cloud_alpha_shape(pcd, alpha)\n",
        "\n",
        "        mesh.compute_vertex_normals()\n",
        "        mesh = mesh.filter_smooth_simple(number_of_iterations=5)\n",
        "\n",
        "        o3d.io.write_triangle_mesh(output_path, mesh)\n",
        "        return True, mesh\n",
        "    except Exception as e:\n",
        "        print(f\"Mesh creation error: {str(e)}\")\n",
        "        return False, None"
      ],
      "metadata": {
        "id": "4Dsm-EYFewP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_miou(predictions, targets, model_cat, affordance):\n",
        "    try:\n",
        "        if isinstance(predictions, np.ndarray):\n",
        "            predictions = torch.from_numpy(predictions)\n",
        "        if isinstance(targets, np.ndarray):\n",
        "            targets = torch.from_numpy(targets)\n",
        "\n",
        "        predictions = predictions.cpu().float()\n",
        "        targets = targets.cpu().float()\n",
        "\n",
        "        if len(predictions.shape) > 1:\n",
        "            predictions = predictions[:, 0]\n",
        "        if len(targets.shape) > 1:\n",
        "            targets = targets[:, 0]\n",
        "\n",
        "        if predictions.shape[0] != targets.shape[0]:\n",
        "            predictions = torch.nn.functional.interpolate(\n",
        "                predictions.unsqueeze(0).unsqueeze(0),\n",
        "                size=targets.shape[0],\n",
        "                mode='linear'\n",
        "            ).squeeze()\n",
        "\n",
        "        pred_mask = (predictions > 0.5).float()\n",
        "        target_mask = (targets > 0.5).float()\n",
        "\n",
        "        pred_mask = pred_mask.to(torch.uint8)\n",
        "        target_mask = target_mask.to(torch.uint8)\n",
        "\n",
        "        intersection = (pred_mask & target_mask).sum().item()\n",
        "        union = (pred_mask | target_mask).sum().item()\n",
        "\n",
        "        iou = intersection / union if union > 0 else 0.0\n",
        "\n",
        "        print(f\"Calculated mIoU for {model_cat} ({affordance}): {iou:.4f}\")\n",
        "        return iou\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating mIoU: {str(e)}\")\n",
        "        return 0.0"
      ],
      "metadata": {
        "id": "bZdd3zM4e3g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_renders_PART3(dir, i, rendered_images, name=None, mesh_id=None):\n",
        "    if mesh_id:\n",
        "        specific_dir = os.path.join(dir, mesh_id, 'renders')\n",
        "    else:\n",
        "        specific_dir = os.path.join(dir, 'renders')\n",
        "\n",
        "    os.makedirs(specific_dir, exist_ok=True)\n",
        "\n",
        "    if name is not None:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(specific_dir, name))\n",
        "    else:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(specific_dir, f'iter_{i}.jpg'))\n",
        "\n",
        "def save_final_results_PART3(log_dir, name, mesh, mlp, vertices, colors, render, background, mesh_id=None):\n",
        "    if mesh_id:\n",
        "        log_dir = os.path.join(log_dir, mesh_id)\n",
        "\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "    mlp.eval()\n",
        "    with torch.no_grad():\n",
        "        probs = mlp(vertices)\n",
        "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
        "\n",
        "        one_hot = torch.zeros(probs.shape).to(device)\n",
        "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
        "\n",
        "        sampled_mesh = mesh\n",
        "\n",
        "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
        "        gray = torch.tensor([180, 180, 180]).to(device)\n",
        "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
        "\n",
        "        color_mesh(one_hot, sampled_mesh, colors)\n",
        "\n",
        "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=5,\n",
        "                                                  show=False,\n",
        "                                                  center_azim=0,\n",
        "                                                  center_elev=0,\n",
        "                                                  std=1,\n",
        "                                                  return_views=True,\n",
        "                                                  lighting=True,\n",
        "                                                  background=background)\n",
        "\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(log_dir, 'final_render.jpg'))\n",
        "\n",
        "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
        "        final_color = torch.where(max_idx==0, highlight, gray)\n",
        "        mesh.export(os.path.join(log_dir, f\"{name}.ply\"), extension=\"ply\", color=final_color)"
      ],
      "metadata": {
        "id": "DZsvbXL7e5rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_single_object(data, targets, model_id, config, validation=False):\n",
        "    try:\n",
        "        mesh_path = os.path.join(\n",
        "            config['output_meshes_dir'],\n",
        "            f\"{config['target_category']}_{model_id}_mesh.obj\"\n",
        "        )\n",
        "\n",
        "        if not point_cloud_to_mesh_part3(data, mesh_path)[0]:\n",
        "            return None\n",
        "\n",
        "        prompt = f\"a 3D rendering of {config['target_category']} show the region highlited optimized for listening\"\n",
        "\n",
        "        if validation:\n",
        "            current_mesh_id = f\"{config['target_category']}_{model_id}\"\n",
        "            output_dir = os.path.join(config['output_results_dir'], current_mesh_id)\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "            os.makedirs(os.path.join(output_dir, 'renders'), exist_ok=True)\n",
        "        else:\n",
        "            output_dir = config['output_results_dir']\n",
        "            current_mesh_id = None\n",
        "\n",
        "        pred_class = optimize_highlighted_regions(\n",
        "            obj_path=mesh_path,\n",
        "            prompt=prompt,\n",
        "            output_dir=config['output_results_dir'],\n",
        "            clip_model_name=config['clip_model_name'],\n",
        "            render_res=config['render_res'],\n",
        "            learning_rate=config['learning_rate'],\n",
        "            n_iter=config['n_iter'],\n",
        "            n_views=config['n_views'],\n",
        "            n_augs=config['n_augs'],\n",
        "            device=config['device'],\n",
        "            mesh_id=current_mesh_id,\n",
        "            is_validation=validation\n",
        "        )\n",
        "\n",
        "        miou = calculate_miou(\n",
        "            predictions=pred_class,\n",
        "            targets=targets[:,:2],\n",
        "            model_cat=config['target_category'],\n",
        "            affordance='Listen'\n",
        "        )\n",
        "\n",
        "        return miou\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing object {model_id}: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "Lr9OUcywe8k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pipeline(config, dataset, dataset_val):\n",
        "    print(f\"Starting optimization for {config['target_category']}\")\n",
        "\n",
        "    # Train phase\n",
        "    train_results = []\n",
        "    max_train_samples = 3\n",
        "    train_indices = [idx for idx, (_, _, _, _, model_cat) in enumerate(dataset)\n",
        "                    if model_cat == config['target_category']]\n",
        "    train_samples = random.sample(train_indices, min(len(train_indices), max_train_samples))\n",
        "\n",
        "    for idx in train_samples:\n",
        "        datas, _, targets, model_id, model_cat = dataset[idx]\n",
        "        if model_cat == config['target_category']:\n",
        "            result = process_single_object(datas, targets, model_id, config, validation=False)\n",
        "            if result is not None:\n",
        "                result_string = f\"{model_id}: {result}\"\n",
        "                train_results.append(result_string)\n",
        "\n",
        "    # Validation phase\n",
        "    val_results = []\n",
        "    val_indices = [idx for idx, (_, _, _, _, model_cat) in enumerate(dataset_val)\n",
        "                  if model_cat == config['target_category']]\n",
        "    val_samples = random.sample(val_indices, min(len(val_indices), 3))\n",
        "\n",
        "    for idx in val_samples:\n",
        "        datas, _, targets, model_id, model_cat = dataset_val[idx]\n",
        "        if model_cat == config['target_category']:\n",
        "            result = process_single_object(datas, targets, model_id, config, validation=True)\n",
        "            if result is not None:\n",
        "                result_string = f\"{model_id}: {result}\"\n",
        "                val_results.append(result_string)\n",
        "\n",
        "    results = {\n",
        "        'train_results': train_results,\n",
        "        'val_results': val_results\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(config['output_dir'], 'optimization_results.json'), 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    return train_results, val_results"
      ],
      "metadata": {
        "id": "IfZeiSw_e-iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'data_dir': \"/content/dataset\",\n",
        "    'output_dir': \"/content/output_PART3\",\n",
        "    'output_meshes_dir': \"/content/output_PART3/meshes\",\n",
        "    'output_results_dir': \"/content/output_PART3/results\",\n",
        "    'target_category': \"Earphone\",\n",
        "    'clip_model_name': 'ViT-B/32',\n",
        "    'render_res': 224,\n",
        "    'learning_rate': 0.0001,\n",
        "    'n_iter': 2500,\n",
        "    'n_views': 5,\n",
        "    'n_augs': 5,\n",
        "    'device': \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "}\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs(config['output_meshes_dir'], exist_ok=True)\n",
        "os.makedirs(config['output_results_dir'], exist_ok=True)\n",
        "\n",
        "# Load datasets\n",
        "dataset_train = AffordNetDataset(data_dir=config['data_dir'], split='train')\n",
        "dataset_val = AffordNetDataset(data_dir=config['data_dir'], split='val')\n",
        "\n",
        "# Run pipeline\n",
        "train_results, val_results = run_pipeline(config, dataset_train, dataset_val)\n",
        "print(f\"train: {train_results}\")\n",
        "print(f\"val: {val_results}\")"
      ],
      "metadata": {
        "id": "RZI5ae24fAm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Extension - Training Augmentation**"
      ],
      "metadata": {
        "id": "VI5sqxjpxCkP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the new function *optimized_highlited_regions* where is applied the extension choosen, where we try different augmentation and calculate new mIoU"
      ],
      "metadata": {
        "id": "xu7nSUZe5iDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_highlighted_regions(\n",
        "    obj_path,\n",
        "    prompt,\n",
        "    output_dir='./output/',\n",
        "    clip_model_name='ViT-B/32',\n",
        "    render_res=224,\n",
        "    res=224,\n",
        "    learning_rate=0.0001,\n",
        "    n_iter=2500,\n",
        "    n_views=5,\n",
        "    n_augs=5,\n",
        "    device='cuda',\n",
        "    mesh_id=None,\n",
        "    is_validation=False\n",
        "):\n",
        "\n",
        "    seed = 0\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "    os.makedirs(os.path.join(output_dir, 'renders'), exist_ok=True)\n",
        "\n",
        "    objbase, extension = os.path.splitext(os.path.basename(obj_path))\n",
        "\n",
        "    render = Renderer(dim=(render_res, render_res))\n",
        "    mesh = Mesh(obj_path)\n",
        "    MeshNormalizer(mesh)()\n",
        "\n",
        "    clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "    clip_transform = transforms.Compose([\n",
        "        transforms.Resize((res, res)),\n",
        "        clip_normalizer\n",
        "    ])\n",
        "\n",
        "    # Initialize CLIP model\n",
        "    augment_params = {\n",
        "    'scale': (0.8, 1.0),\n",
        "    'brightness': 0.4,\n",
        "    'contrast': 0.4,\n",
        "    'saturation': 0.4,\n",
        "    'hue': 0.2,\n",
        "    'distortion_scale': 0.5\n",
        "    }\n",
        "\n",
        "    augment_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(res, scale=augment_params['scale']),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=augment_params['distortion_scale']),\n",
        "        transforms.ColorJitter(\n",
        "            brightness=augment_params['brightness'],\n",
        "            contrast=augment_params['contrast'],\n",
        "            saturation=augment_params['saturation'],\n",
        "            hue=augment_params['hue']\n",
        "        ),\n",
        "        transforms.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 2.0)),\n",
        "        clip_normalizer\n",
        "    ])\n",
        "\n",
        "\n",
        "    clip_model, preprocess = get_clip_model(clip_model_name)\n",
        "    with torch.no_grad():\n",
        "        prompt_token = clip.tokenize([prompt]).to(device)\n",
        "        encoded_text = clip_model.encode_text(prompt_token)\n",
        "        encoded_text = encoded_text / encoded_text.norm(dim=1, keepdim=True)\n",
        "\n",
        "    # MLP Settings\n",
        "    mlp = NeuralHighlighter().to(device)\n",
        "    optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n",
        "\n",
        "    rgb_to_color = {(204/255, 1., 0.): \"highlighter\", (180/255, 180/255, 180/255): \"gray\"}\n",
        "    color_to_rgb = {\"highlighter\": [204/255, 1., 0.], \"gray\": [180/255, 180/255, 180/255]}\n",
        "    full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]\n",
        "    colors = torch.tensor(full_colors).to(device)\n",
        "\n",
        "    background = torch.tensor((0., 0., 0.)).to(device)\n",
        "\n",
        "    vertices = copy.deepcopy(mesh.vertices)\n",
        "    losses = []\n",
        "\n",
        "    # OOPTIMIZATION LOOP:\n",
        "    for i in tqdm(range(n_iter)):\n",
        "        optim.zero_grad()\n",
        "\n",
        "        pred_class = mlp(vertices)\n",
        "\n",
        "        sampled_mesh = mesh\n",
        "        color_mesh(pred_class, sampled_mesh, colors)\n",
        "        rendered_images, elev, azim = render.render_views(sampled_mesh, num_views=n_views,\n",
        "                                                          show=False,\n",
        "                                                          center_azim=0,\n",
        "                                                          center_elev=0,\n",
        "                                                          std=1,\n",
        "                                                          return_views=True,\n",
        "                                                          lighting=True,\n",
        "                                                          background=background)\n",
        "\n",
        "        # Calculate CLIP Loss\n",
        "        loss = clip_loss(rendered_images, clip_model, encoded_text, clip_transform, augment_transform, n_augs, clipavg='view')\n",
        "        loss.backward(retain_graph=True)\n",
        "\n",
        "        optim.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        if is_validation and i % 100 == 0:\n",
        "            save_renders_PART3(output_dir, i, rendered_images, mesh_id=mesh_id)\n",
        "            with open(os.path.join(output_dir, \"training_info.txt\"), \"a\") as f:\n",
        "                f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n",
        "\n",
        "    if is_validation:\n",
        "        save_final_results_PART3(output_dir, objbase, mesh, mlp, vertices, colors, render, background, mesh_id = mesh_id)\n",
        "\n",
        "\n",
        "    return pred_class"
      ],
      "metadata": {
        "id": "s-TeIFbcxLKT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}